{
    "name": "scs",
    "aliases": [],
    "versions": [
        {
            "name": "2.1.1",
            "sha256": "0e20b91e8caf744b84aa985ba4e98cc7235ee33612b2bad2bf31ea5ad4e07d93"
        }
    ],
    "latest_version": "2.1.1",
    "build_system": "MakefilePackage",
    "conflicts": [
        {
            "name": "platform=windows",
            "spec": "build_system=makefile",
            "description": "None"
        }
    ],
    "variants": [
        {
            "name": "build_system",
            "default": "makefile",
            "description": "Build systems supported by the package"
        },
        {
            "name": "cuda",
            "default": false,
            "description": "Build with Cuda support"
        }
    ],
    "homepage": "https://github.com/cvxgrp/scs",
    "maintainers": [],
    "patches": [
        {
            "owner": "builtin.scs",
            "sha256": "314e53d7d97b6f010b98a2dacd85764a332001da1538e65102d84f25e2805760",
            "level": 1,
            "working_dir": ".",
            "relative_path": "make_gpu.patch",
            "version": ""
        }
    ],
    "resources": [],
    "description": "A C package that solves convex cone problems via operator splitting\n",
    "dependencies": [
        {
            "name": "blas",
            "description": "XBLAS is a reference implementation for extra precision BLAS. XBLAS is a\nreference implementation for the dense and banded BLAS routines, along\nwith extended and mixed precision version. Extended precision is only\nused internally; input and output arguments remain the same as in the\nexisting BLAS. Extra precisions is implemented as double-double (i.e.,\n128-bit total, 106-bit significand). Mixed precision permits some\ninput/output arguments of different types (mixing real and complex) or\nprecisions (mixing single and double). This implementation is proof of\nconcept, and no attempt was made to optimize performance; performance\nshould be as good as straightforward but careful code written by hand."
        },
        {
            "name": "lapack",
            "description": "libFLAME (AMD Optimized version) is a portable library for dense matrix\ncomputations, providing much of the functionality present in Linear\nAlgebra Package (LAPACK). It includes a compatibility layer, FLAPACK,\nwhich includes complete LAPACK implementation. The library provides\nscientific and numerical computing communities with a modern, high-\nperformance dense linear algebra library that is extensible, easy to\nuse, and available under an open source license. libFLAME is a C-only\nimplementation and does not depend on any external FORTRAN libraries\nincluding LAPACK. There is an optional backward compatibility layer,\nlapack2flame that maps LAPACK routine invocations to their corresponding\nnative C implementations in libFLAME. This allows legacy applications to\nstart taking advantage of libFLAME with virtually no changes to their\nsource code. In combination with BLIS library which includes\noptimizations for the AMD EPYC processor family, libFLAME enables\nrunning high performing LAPACK functionalities on AMD platform.\nLICENSING INFORMATION: By downloading, installing and using this\nsoftware, you agree to the terms and conditions of the AMD AOCL-libFLAME\nlicense agreement. You may obtain a copy of this license agreement from\nhttps://www.amd.com/en/developer/aocl/blis/libflame-4-0-eula.html\nhttps://www.amd.com/en/developer/aocl/blis/libflame-eula.html"
        },
        {
            "name": "cuda",
            "description": "CUDA is a parallel computing platform and programming model invented by\nNVIDIA. It enables dramatic increases in computing performance by\nharnessing the power of the graphics processing unit (GPU). Note: This\npackage does not currently install the drivers necessary to run CUDA.\nThese will need to be installed manually. See:\nhttps://docs.nvidia.com/cuda/ for details."
        }
    ],
    "dependent_to": []
}