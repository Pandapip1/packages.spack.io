{
    "name": "aws-ofi-nccl",
    "aliases": [],
    "versions": [
        {
            "name": "master",
            "branch": "master",
            "default": true
        }
    ],
    "latest_version": "master",
    "build_system": "AutotoolsPackage",
    "conflicts": [
        {
            "name": "platform=windows",
            "spec": "build_system=autotools",
            "description": "None"
        }
    ],
    "variants": [
        {
            "name": "build_system",
            "default": "autotools",
            "description": "Build systems supported by the package"
        },
        {
            "name": "trace",
            "default": false,
            "description": "Enable printing trace messages"
        },
        {
            "name": "tests",
            "default": false,
            "description": "Build tests"
        }
    ],
    "homepage": "https://github.com/aws/aws-ofi-nccl",
    "maintainers": [
        "bvanessen"
    ],
    "patches": [],
    "resources": [],
    "description": "AWS OFI NCCL is a plug-in which enables EC2 developers to use libfabric\nas a network provider while running NVIDIA's NCCL based applications.\n",
    "dependencies": [
        {
            "name": "gnuconfig",
            "description": "The GNU config.guess and config.sub scripts versioned by timestamp.\nThis package can be used as a build dependency for autotools packages\nthat ship a tarball with outdated config.guess and config.sub files."
        },
        {
            "name": "libfabric",
            "description": "The Open Fabrics Interfaces (OFI) is a framework focused on exporting\nfabric communication services to applications."
        },
        {
            "name": "cuda",
            "description": "CUDA is a parallel computing platform and programming model invented by\nNVIDIA. It enables dramatic increases in computing performance by\nharnessing the power of the graphics processing unit (GPU). Note: This\npackage does not currently install the drivers necessary to run CUDA.\nThese will need to be installed manually. See:\nhttps://docs.nvidia.com/cuda/ for details."
        },
        {
            "name": "nccl",
            "description": "Optimized primitives for collective multi-GPU communication."
        },
        {
            "name": "mpi",
            "description": "Virtual package for the Message Passing Interface."
        },
        {
            "name": "autoconf",
            "description": "Autoconf -- system configuration part of autotools"
        },
        {
            "name": "automake",
            "description": "Automake -- make file builder part of autotools"
        },
        {
            "name": "libtool",
            "description": "libtool -- library building part of autotools."
        }
    ],
    "dependent_to": [
        {
            "name": "aluminum",
            "description": "Aluminum provides a generic interface to high-performance communication\nlibraries, with a focus on allreduce algorithms. Blocking and non-\nblocking algorithms and GPU-aware algorithms are supported. Aluminum\nalso contains custom implementations of select algorithms to optimize\nfor certain situations."
        }
    ]
}