{
    "name": "py-tensorflow-metadata",
    "aliases": [],
    "versions": [
        {
            "name": "1.5.0",
            "sha256": "f0ec8aaf62fd772ef908efe4ee5ea3bc0d67dcbf10ae118415b7b206a1d61745"
        }
    ],
    "build_system": "PythonPackage",
    "conflicts": [],
    "variants": [],
    "homepage": "https://pypi.org/project/tensorflow-metadata/",
    "maintainers": [
        "adamjstewart"
    ],
    "patches": [],
    "resources": [],
    "description": "Library and standards for schema and statistics. TensorFlow Metadata\nprovides standard representations for metadata that are useful when\ntraining machine learning models with TensorFlow.\n",
    "dependencies": [
        {
            "name": "python",
            "description": "The Python programming language."
        },
        {
            "name": "bazel",
            "description": "Bazel is an open-source build and test tool similar to Make, Maven, and\nGradle. It uses a human-readable, high-level build language. Bazel\nsupports projects in multiple languages and builds outputs for multiple\nplatforms. Bazel supports large codebases across multiple repositories,\nand large numbers of users."
        },
        {
            "name": "py-setuptools",
            "description": "A Python utility that aids in the process of downloading, building,\nupgrading, installing, and uninstalling Python packages."
        },
        {
            "name": "py-absl-py",
            "description": "This repository is a collection of Python library code for building\nPython applications. The code is collected from Google's own Python code\nbase, and has been extensively tested and used in production."
        },
        {
            "name": "py-googleapis-common-protos",
            "description": "Common protobufs used in Google APIs."
        },
        {
            "name": "py-protobuf",
            "description": "Protocol buffers are Google's language-neutral, platform-neutral,\nextensible mechanism for serializing structured data - think XML, but\nsmaller, faster, and simpler. You define how you want your data to be\nstructured once, then you can use special generated source code to\neasily write and read your structured data to and from a variety of data\nstreams and using a variety of languages."
        }
    ],
    "dependent_to": [
        {
            "name": "py-tensorflow-datasets",
            "description": "tensorflow/datasets is a library of datasets ready to use with\nTensorFlow."
        }
    ]
}