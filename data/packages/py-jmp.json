{
    "name": "py-jmp",
    "aliases": [],
    "versions": [
        {
            "name": "0.0.2",
            "sha256": "4d242fb14502b15a7c072e112bdcd7cb5d8b373d9733162eea23e0b9b7dbb6d0"
        }
    ],
    "latest_version": "0.0.2",
    "build_system": "PythonPackage",
    "conflicts": [],
    "variants": [
        {
            "name": "build_system",
            "default": "python_pip",
            "description": "Build systems supported by the package"
        }
    ],
    "homepage": "https://github.com/deepmind/jmp",
    "maintainers": [
        "adamjstewart"
    ],
    "patches": [],
    "resources": [],
    "description": "JMP is a Mixed Precision library for JAX.\n",
    "dependencies": [
        {
            "name": "python",
            "description": "The Python programming language."
        },
        {
            "name": "py-pip",
            "description": "The PyPA recommended tool for installing Python packages."
        },
        {
            "name": "py-wheel",
            "description": "A built-package format for Python."
        },
        {
            "name": "py-setuptools",
            "description": "A Python utility that aids in the process of downloading, building,\nupgrading, installing, and uninstalling Python packages."
        },
        {
            "name": "py-numpy",
            "description": "NumPy is the fundamental package for scientific computing with Python.\nIt contains among other things: a powerful N-dimensional array object,\nsophisticated (broadcasting) functions, tools for integrating C/C++ and\nFortran code, and useful linear algebra, Fourier transform, and random\nnumber capabilities"
        },
        {
            "name": "py-jax",
            "description": "JAX is Autograd and XLA, brought together for high-performance machine\nlearning research. With its updated version of Autograd, JAX can\nautomatically differentiate native Python and NumPy functions. It can\ndifferentiate through loops, branches, recursion, and closures, and it\ncan take derivatives of derivatives of derivatives. It supports reverse-\nmode differentiation (a.k.a. backpropagation) via grad as well as\nforward-mode differentiation, and the two can be composed arbitrarily to\nany order."
        }
    ],
    "dependent_to": [
        {
            "name": "py-dm-haiku",
            "description": "JAX-based neural network library"
        }
    ]
}